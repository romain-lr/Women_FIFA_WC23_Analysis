---
title: "Projet_Women_FIFA_WC23_Analysis"
institute : "INSA Toulouse"
date: "`r Sys.Date()`"
always_allow_html: true
output: 
  pdf_document :
    toc : TRUE
    toc_depth : 3
    number_section : TRUE
    fig_caption: yes
header-includes:
   - \usepackage{dsfont}
   - \usepackage{color}
   - \newcommand{\1}{\mathds{1}}
---

```{r, include=FALSE}
#Eval=False : on compile pas le chunk
#Include=False : on affiche rien
#Echo=False : on affiche que la sortie
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(StatsBombR)
library(extrafont)
library(MASS)
library(dplyr)
library(plotly)
library(stringr)
library(ggplot2)
library(writexl)
library(readxl)
library(gridExtra)
library(matrixStats)
library(ggplot2)




```

\newpage

# Introduction

  Blalblablabla
  
  
```{r, include=FALSE}
# World Cup 2023 (id 72)
WC2023 <- FreeCompetitions() %>%
filter(competition_id==72 & season_name=="2023")

# Games availables in WC2023
Matches <- FreeMatches(WC2023)
```

```{r, include=FALSE,eval=F}

# Events for the games of the WC2023
WC2023_dataframe <- free_allevents(MatchesDF = Matches, Parallel = T)
WC2023_dataframe = allclean(WC2023_dataframe)

```


```{r, include=F}

# Lire le fichier CSV dans R
#write.csv(WC2023_dataframe, "WC2023_dataframe.csv", row.names = FALSE)

WC2023_dataframe <- read.csv("WC2023_dataframe.csv")

```



```{r,, include=FALSE}
WC2023_dataframe <- WC2023_dataframe %>%
  mutate(team.name = str_remove(team.name, " Women's"))

WC2023_dataframe <- WC2023_dataframe %>%
  mutate(possession_team.name = str_remove(possession_team.name, " Women's"))
```

# Descriptive data analysis 

 We begin by interpreting the elements of the data set.\
It is made up of multiple observations with different variables.\

## Analysis of successful shots according to country
   First, we look at the number of goals and shots in all matches for each team.\
Figure \ref{fig:fig1} shows a visualization of these results.\


```{r, include=FALSE}
#Number of goals and shots in all games for each team
shots_goals = WC2023_dataframe %>%
group_by(team.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 
```

```{r fig1 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig1}Diagram of the number of goals and shots in all matches for each team",fig.height=4.5}
#Let's make a graph
ggplot(shots_goals, aes(y = reorder(as.factor(team.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of goals and shots in all games for each team",
       y = "Team",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
```

It would be interesting to make this graph on the average number of goals and shots, as some teams have more games than others, distorting the results a little.

Figure \ref{fig:fig2} shows a visualization of the percentage of shots leading to a goal.\

```{r fig2 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig2}Diagram of the percentage of shots leading to a goal",fig.height=4,fig.align='center'}

ggplot(shots_goals, aes(y = reorder(as.factor(team.name), goals/shots*100))) +
  geom_bar(aes(x = goals/shots*100), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Percentage of shots leading to a goal",
       y = "Team",
       x = "Percentage")
```

\vspace{3pt}

We now would like to focus on France team.\
Figure \ref{fig:fig3} shows the results.\

```{r,include=F}
#Number of goals and shots in each match for both teams
shots_goals_all_matches = WC2023_dataframe %>%
group_by(match_id) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

```

```{r,include=F}
#We display ids of all France matches
Id_France = WC2023_dataframe%>% filter(team.name=="France")
match_ids_france=unique(Id_France$match_id)
print(match_ids_france)
```


```{r,include=F}

#For each French match, the total number of shots and goals for the 2 teams (France+adversary) is displayed.


france_goals <- shots_goals_all_matches %>%
  filter(match_id %in% match_ids_france)

print(france_goals)
```

```{r,include=F}

ggplot(france_goals, aes(y = as.factor(match_id))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of shots and goals for each French match",
       y = "Match_id",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
  
```



```{r,include=F}
ggplot(france_goals, aes(y = reorder(as.factor(match_id), goals/shots*100))) +
  geom_bar(aes(x = goals/shots*100), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Percentage of shots on goal in France matches",
       y = "Match",
       x = "Percentage")

#Il faut ordonner par date pour avoir l'évolution de la stratégie au cours de la compétition


```

```{r,include=F}

#We want to know which goals come from France
shots_goals_France_all_matches = WC2023_dataframe %>%
filter(team.name=="France") %>%
group_by(match_id) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

print (shots_goals_France_all_matches)

```



```{r fig3 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig3}Diagram of the number of shots and goals for each French and percentage",fig.height=2,fig.align='center'}

g1=ggplot(shots_goals_France_all_matches, aes(y = as.factor(match_id))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Numbers for French matches",
       y = "Match_id",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green")) 


g2=ggplot(shots_goals_France_all_matches, aes(y = as.factor(match_id))) +
  geom_bar(aes(x = goals/shots*100), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Percentage in France matches",
       y = "Match",
       x = "Percentage") 

#Order from bottom to top: 1st match to last
grid.arrange(g1,g2,ncol=2)
```
\clearpage
 
## Analysis of successful shots according to different variables  

We first look at the type of shots.\

```{r,include=F}
#Number of goals and shots in all matches by type of shot
shots_goals_formation = WC2023_dataframe %>%
group_by(shot.type.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

shots_goals_formation=shots_goals_formation[-c(5),]
```

```{r fig4 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig4}Diagram of the number of shots and goals for each type of shot",fig.height=2,fig.align='center'}
ggplot(shots_goals_formation, aes(y = reorder(as.factor(shot.type.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of shots and goals for each type of shot",
       y = "type",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
```


  We know would like to see if the technique of shot is significant.\

```{r,include=F}
#Number of shots and goals for each technique of shot
shots_goals_technique = WC2023_dataframe %>%
group_by(shot.technique.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

#On enlève la ligne NA

shots_goals_technique=shots_goals_technique[-c(8),]
```

```{r fig5 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig5}Diagram of the number of shots and goals for each technique of shot",fig.height=2,fig.pos="h",fig.align='center'}

ggplot(shots_goals_technique, aes(y = reorder(as.factor(shot.technique.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of shots and goals for each technique of shot",
       y = "Technique of shot",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
```

Finally, is the variable body_zone_used relevant ?

```{r,include=F}
#Number of goals and shots over all matches according to body zone used
shots_goals_part = WC2023_dataframe %>%
group_by(shot.body_part.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 
shots_goals_part=shots_goals_part[-c(5),]

```

```{r fig6 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig6}Diagram of the number of goals and shots according to body zone used",fig.height=2,fig.pos="h"}

ggplot(shots_goals_part, aes(y = reorder(as.factor(shot.body_part.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of goals and shots according to body zone used",
       y = "Body zone",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green")) 
```
\clearpage

# Models analysis

We wanted to create our own xG model. To do that we developed different models, finding the most relevant variables to predict goals.\

```{r,include=F}
#We create a df with just the shots

shots = WC2023_dataframe %>%
 filter(type.name=="Shot")
```

We run a logistic regression model: we want the output to be 0 or 1 depending on whether the shot turns into a goal.\

The first model keeps the variables studied previously : body part, technique, type of shot. \


```{r,include=F}

df_model_1 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name)
df_model_1$shot.outcome.name <- ifelse(df_model_1$shot.outcome.name == "Goal", 1, 0)


```

```{r, include = F}
#Testing regression without interaction

glm.model_1<-glm(shot.outcome.name~ . ,data=df_model_1,family=binomial(link="logit"))
summary(glm.model_1)
```

```{r,include=F}
#We see a skewed shot.type.namePenalty because the corner modality is taken as reference and, given that there were no shots or goals during the competition, the model considers 100% success for the corner modality.


#We change the reference modality from “corner” to “penalty”.
df_model_1_modif=df_model_1
df_model_1_modif$shot.type.name <- relevel(as_factor(df_model_1_modif$shot.type.name), ref = "Penalty")

glm.model_1_modif <- glm(shot.outcome.name ~ . , data = df_model_1_modif, family = binomial(link = "logit"))
summary(glm.model_1_modif)

```

```{r,include=F}
#We can see that Open Play and Free Kick are less likely to result in a goal than a penalty kick. The corner remains distorted
```




```{r,include=F}
#R2 calculation:
R2_model1=1-(glm.model_1$deviance/glm.model_1$null.deviance)
print(R2_model1)

```
$R^2$ for the model without interaction is : \( `r R2_model1` \) \
```{r,include=F}
#Testing regression with interaction
glm.model_1_interac<-glm(shot.outcome.name~ .^2 ,data=df_model_1,family=binomial(link="logit"))
summary(glm.model_1_interac)
#A lot of NA because some interactions are impossible (e.g. volleyball restart on a penalty kick).

```


```{r,include=F}
#R2 calculation:
R2_model1_interac=1-(glm.model_1_interac$deviance/glm.model_1_interac$null.deviance)
print(R2_model1_interac)
```

$R^2$ for the model with interaction is : \( `r R2_model1_interac` \) 
We make a model with the given expected goal as variable.\


```{r,include=F}
df_model_2 <- dplyr::select(shots,shot.statsbomb_xg,shot.outcome.name)
df_model_2$shot.outcome.name <- ifelse(df_model_2$shot.outcome.name == "Goal", 1, 0)

glm.model_2<-glm(shot.outcome.name~ . ,data=df_model_2,family=binomial(link="logit"))
summary(glm.model_2)

```

```{r,include=F}
R2_model2=1-(glm.model_2$deviance/glm.model_2$null.deviance)
print(R2_model2)

```

We would therefore like to find a model with an $R^2$ value close to this model, i.e. an $R^2$ close to : \( `r R2_model2` \)

We do the same logistical model but with the position added : location.x and location.y

```{r,include=F}

df_model_3 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y)
df_model_3$shot.outcome.name <- ifelse(df_model_3$shot.outcome.name == "Goal", 1, 0)

```

```{r,include=F}
#Model without interaction
glm.model_3<-glm(shot.outcome.name~ . ,data=df_model_3,family=binomial(link="logit"))
summary(glm.model_3)
```


```{r,include=F}
R2_model_3=1-(glm.model_3$deviance/glm.model_3$null.deviance)
print(R2_model_3)

```
We test a regression without interaction, and obtain an $R^2$ of : \( `r R2_model_3` \)


```{r,include=F}
#Model with interaction

glm.model_3_interac<-glm(shot.outcome.name~ .^2 ,data=df_model_3,family=binomial(link="logit"))
summary(glm.model_3_interac)
```

```{r,include=F}
R2_model_3_interac=1-(glm.model_3_interac$deviance/glm.model_3_interac$null.deviance)
print(R2_model_3_interac)
```

With interactions, we get an $R^2$ of : \( `r R2_model_3_interac` \)

In this model, we targeted the main variables to obtain a good model and an $R^2$ as close to 1 as possible.


We run several tests to see which variables are significant in the model. 

```{r,echo=F}

model_simp<-glm(shot.outcome.name~ (location.x +location.y + shot.body_part.name+shot.type.name)^2 ,data=df_model_3,family=binomial(link="logit"))
anova(model_simp,glm.model_3_interac,test="Chisq")

```

We see that we can remove the technique because $p-value$>0.05 so we can accept the sub-model with a 95% level.

```{r,include=F}
summary(model_simp)
```


```{r,include=F}
R2_model_simp=1-(model_simp$deviance/model_simp$null.deviance)
print(R2_model_simp)
```

For this model we obtain an $R^2$ of : \( `r R2_model_simp` \)

The $R^2$ is no greater than for model 3 with interactions: this is normal because the $R^2$ favors models with many variables.
We should look at other variables such as AIC, which is minimum for model 3 without interactions.

```{r,include=F}
#We'll compare the fitted values (probability of being a goal), with the expected goals.
glm.model_3$fitted.values
#we have numbers between 0 and 1 so this is good.

```

We now want to compare model 3 with and without interaction : the closer the 2-norm is to 0, the better the model. 


```{r,include=F}

#Display norm L2 for the model_3 without interaction

norm_L2_mod3 <- norm(glm.model_3$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod3)

#Display norm L2 for the model_3 with interactions 

norm_L2_mod3withinteractions <- norm(glm.model_3_interac$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod3withinteractions)


```
Norm L2 for the model_3 without interaction is equal to : \( `r norm_L2_mod3` \). \

The value for the model_3 with interactions is : \( `r norm_L2_mod3withinteractions` \).\

We find the same results as with the AIC criterion. This is consistent with the fact that $R^2$ favors models with many variables, so it's better to evaluate with AIC. The model 3 without interaction is best.

We do the same to compare model 1 with and without interaction.

```{r,include=F}

#Display norm L2 for the model_1 without interaction
norm_L2_mod1 <- norm(glm.model_1$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod1)

#Display norm L2 for the model_1 with interactions
norm_L2_mod1withinteractions <- norm(glm.model_1_interac$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod1withinteractions)

```

The L2 norms are respectively :  \( `r norm_L2_mod1` \) and \( `r norm_L2_mod1withinteractions` \)

Both models are less accurate than the 3rd one.


We now create a new model like the model_3, but adding a variable : under_pressure.
First, we test the significance of this new variable.

```{r,echo=F}
df_model_4 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y,under_pressure)
df_model_4$shot.outcome.name <- ifelse(df_model_4$shot.outcome.name == "Goal", 1, 0)

#Replace NA with “false” in the under_pressure column
df_model_4$under_pressure <- ifelse(is.na(df_model_4$under_pressure), FALSE, df_model_4$under_pressure)

#Testing the model with only the under_pressure variable
glm.model_4 <- glm(shot.outcome.name ~ under_pressure, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4)
```


We see that \( p_{\text{value}} < 0.05 \), so we reject $H_0$ : playing under pressure is significant.

Estimated coefficients are negative, so playing under pressure reduces the probability of scoring. 




```{r,include=F}
glm.model_4_bodypart <- glm(shot.outcome.name ~ shot.body_part.name, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_bodypart)
```


Testing the model with only the shot.body_part.name variable gives us a $p_{value}$ of :
\( `r summary(glm.model_4_bodypart)$coefficients[,"Pr(>|z|)"]` \)

The probability of marking the head is lower than for other parts of the body.

We now want to test the model with only the shot.technique.name variable.


```{r,echo=F}

glm.model_4_technique <- glm(shot.outcome.name ~ shot.technique.name, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_technique)
```

The reference is backheel (tallonade): all the other techniques are better, we have a lot of values close to 1, we could do a constant sub-model to see if this variable is significant.

```{r,include=F}
model_constant= glm(shot.outcome.name ~ 1, data = df_model_4, family = binomial(link = "logit"))
anova(model_constant,glm.model_4_technique,test="Chisq")
```
We find a $p_{value}$ of \( `r round(anova(model_constant, glm.model_4_technique, test="Chisq")$Pr[2], digits = 3)` \).
We reject $H_0$, the technique variable is significant.

We are now testing the model with only the shot.type.name variable. We also run a sub-model test.


```{r,include=F}

glm.model_4_type <- glm(shot.outcome.name ~ shot.type.name, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_type)

```

```{r,include=F}
anova(model_constant,glm.model_4_type,test="Chisq")
```
We find a $p_{value}$ of \( `r round(anova(model_constant, glm.model_4_type, test="Chisq")$Pr[2], digits = 3)` \).

The variable type.name is significant, we reject $H_0$.

We do the same with the variable location.x : 

```{r,include=F}
glm.model_4_loc_x <- glm(shot.outcome.name ~ location.x, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_loc_x)
```

We see a $p_{value}$ of : \( `r summary(glm.model_4_bodypart)$coefficients[,"Pr(>|z|)"]` \) <0.05 so location.x is highly significant.\

#------------------------------------------------------

The model is now tested with all the following variables: shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y,under_pressure
```{r,include=F}
#We test now with all the variable in the model 3+under_pressure 
glm.model_5 <- glm(shot.outcome.name ~ ., data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_5)
AIC_model5=AIC(glm.model_5)
R2_model5=1-(glm.model_5$deviance/glm.model_5$null.deviance)
print(R2_model5)
```

We have an $R^2$ of \( `r R2_model5` \) which is good, but it's normal because it's a model with many variables.
We also note a low AIC, which is equal to \texttt{`r AIC_model5`}.\

We create the same model as above, but adding the position of the goalkeeper. First we test the model with only the location.x.GK variable.
```{r,include=F}
#Create a new df with all previous variables + goalkeeper position
df_model_6 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y,under_pressure,location.x.GK,location.y.GK)
df_model_6$shot.outcome.name <- ifelse(df_model_6$shot.outcome.name == "Goal", 1, 0)

#Replace NA with “false” in the under_pressure column
df_model_6$under_pressure <- ifelse(is.na(df_model_6$under_pressure), FALSE, df_model_6$under_pressure)

#Testing the model with only the location.x.GK variable
glm.model_6_location.x.GK <- glm(shot.outcome.name ~ location.x.GK, data = df_model_6, family = binomial(link = "logit"))
summary(glm.model_6_location.x.GK)

AIC_model6_locx_GK=AIC(glm.model_6_location.x.GK)

print(1-(glm.model_6_location.x.GK$deviance/glm.model_6_location.x.GK$null.deviance))
```

Significant effect of goal position in x.
The AIC value is low, equals to \texttt{`r AIC_model6_locx_GK`}.\

Then we do the same but with the location.y.GK variable.\

```{r,include=F}
#We test the model with only the variable location.y.GK
glm.model_6_location.y.GK <- glm(shot.outcome.name ~ location.y.GK, data = df_model_6, family = binomial(link = "logit"))
summary(glm.model_6_location.y.GK)

AIC_model6_locy_GK=AIC(glm.model_6_location.y.GK)


R2_model6loc_y_GK=1-(glm.model_6_location.y.GK$deviance/glm.model_6_location.y.GK$null.deviance)
print(R2_model6loc_y_GK)
```
Variable for keeper position in y significant.
AIC is slightly higher than for position in x, it's equal to \texttt{`r AIC_model6_locy_GK`}.\
```{r,include=F}
#Testing the model with all df6 variables without interaction
glm.model_6 <- glm(shot.outcome.name ~ ., data = df_model_6, family = binomial(link = "logit"))
summary(glm.model_6)

AIC_model6=AIC(glm.model_6)

R2_model6=1-(glm.model_6$deviance/glm.model_6$null.deviance)
print(R2_model6)

```
Very low AIC=\texttt{`r AIC_model6`}. We can conclude that this model is really good.\

```{r,include=F}
#We create a new df without location.y
df_model_7 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,under_pressure,location.x.GK,location.y.GK)
df_model_7$shot.outcome.name <- ifelse(df_model_7$shot.outcome.name == "Goal", 1, 0)

#Replace NA with “false” in the under_pressure column
df_model_7$under_pressure <- ifelse(is.na(df_model_7$under_pressure), FALSE, df_model_7$under_pressure)

#Test the model with all variables
glm.model_7 <- glm(shot.outcome.name ~ ., data = df_model_7, family = binomial(link = "logit"))
summary(glm.model_7)

AIC_model7=AIC(glm.model_7)

R2_model7=1-(glm.model_7$deviance/glm.model_7$null.deviance)
print(R2_model7)

```

We can therefore remove the variable location.y
The AIC is even lower, at \texttt{`r AIC_model7`}.\
-> Best model for now

```{r,echo=F}

#We want to compare our xg obtained with model 7 with those of statsbomb :

#We have a size problem between the two vectors: there are missing values
#We obtain the indices of missing observations for all variables in model 7

indices_obs_manquantes <- which(!complete.cases(df_model_7))
indices_non_manquants <- setdiff(1:nrow(df_model_7), indices_obs_manquantes)

#Only rows with no missing observations are kept
statsbomb_xg <- shots$shot.statsbomb_xg[indices_non_manquants]

df_plot <- data.frame(statsbomb_xg = statsbomb_xg, fitted = glm.model_7$fitted.values)

#We compare both
ggplot(df_plot, aes(x = seq_along(statsbomb_xg))) +
  geom_point(aes(y = statsbomb_xg), color = "blue") +
  geom_point(aes(y = fitted), color = "red") +
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "Observations",
       y = "Values") +
  theme_minimal()

ggplot(df_plot, aes(x = seq_along(statsbomb_xg))) +
  geom_point(aes(y = log(statsbomb_xg)), color = "blue") +
  geom_point(aes(y = log(fitted)), color = "red") +
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "Observations",
       y = "Values") +
  theme_minimal()


#We plot one against the other: xg against our fitted values
ggplot(df_plot, aes(x = statsbomb_xg, y = fitted)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "statsbomb_xg",
       y = "Fitted values") + 
  theme_minimal()  

#Same as transforming to log
ggplot(df_plot, aes(x = log(statsbomb_xg), y = log(fitted))) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "statsbomb_xg",
       y = "Fitted values") + 
  theme_minimal()  


```

We have a lot of values close to 0, so we do the log to make things clearer.

In log: there's a point (an observation) where we've overestimated the chance of scoring, there are a few points at the bottom right where, on the contrary, we've underestimated the probability, but overall we've got a good prediction based on the Xg of bomb stats.

```{r,echo=F}
#Find the best model (that minimizes AIC)
stepAIC(glm.model_6, trace=FALSE)
```

You can see that y-positions are useless, only x-positions are significant, as well as the body part and the technique used.
-> Best model 

