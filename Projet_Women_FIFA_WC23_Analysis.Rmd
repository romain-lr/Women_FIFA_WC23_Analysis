---
title: "Projet_Women_FIFA_WC23_Analysis"
institute : "INSA Toulouse"
date: "`r Sys.Date()`"
always_allow_html: true
output: 
  pdf_document :
    toc : TRUE
    toc_depth : 3
    number_section : TRUE
    fig_caption: yes
header-includes:
   - \usepackage{dsfont}
   - \usepackage{color}
   - \newcommand{\1}{\mathds{1}}
---

```{r, include=FALSE}
#Eval=False : on compile pas le chunk
#Include=False : on affiche rien
#Echo=False : on affiche que la sortie
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(StatsBombR)
library(extrafont)
library(MASS)
library(dplyr)
library(plotly)
library(stringr)
library(ggplot2)
library(writexl)
library(readxl)
library(gridExtra)
library(matrixStats)
library(ggplot2)




```

\newpage

# Introduction

  In today's world, sports are at the center of global culture. To continue to excel, players and teams must find solutions, both physical and tactical. Therefore, statistics will play a crucial role in optimizing performance. Previous studies have shed light on various aspects of football analytics. Collet studied the impact of possession in 2013. More recently Liu analyzed the environmental impact in 2021. However, the realm of soccer remains relatively untapped in terms of data exploration. Understanding the dynamics of offensive and defensive play is pivotal for teams aiming to excel in competitions. 

The research gap lies in the need for a comprehensive analysis of football performance using advanced statistical methods, with a focus on data from platforms like StatsBomb. The impact of certain specific aspects of football analytics, such as shot analysis or passing patterns, remains unclear, and a comprehensive understanding of player and team performance is still lacking.

We aimed to address this gap by conducting a detailed analysis of football performance using StatsBomb data. We seeked to identify key performance indicators, assess their impact on match outcomes, and uncover underlying trends and patterns in player and team performance. This report outlines the methodology used for data collection and analysis, presents the findings from the study, and discusses their implications for the future of football analytics.

This report is divided into three parts. In the first section, we conduct an exploratory data analysis to identify certain trends, notably by analyzing shots and goals for each team. Then we seek an optimal statistical model to determine which parameters have the greatest impact on player performance. The last section contains the results of our analysis, including insights into player and team performance derived from StatsBomb data, with graphs examining successful shots and passes. 

  
  
```{r, include=FALSE}
# World Cup 2023 (id 72)
WC2023 <- FreeCompetitions() %>%
filter(competition_id==72 & season_name=="2023")

# Games availables in WC2023
Matches <- FreeMatches(WC2023)
```

```{r, include=FALSE,eval=F}

# Events for the games of the WC2023
WC2023_dataframe <- free_allevents(MatchesDF = Matches, Parallel = T)
WC2023_dataframe = allclean(WC2023_dataframe)

```


```{r, include=F}

# Lire le fichier CSV dans R
#write.csv(WC2023_dataframe, "WC2023_dataframe.csv", row.names = FALSE)

WC2023_dataframe <- read.csv("WC2023_dataframe.csv")

```



```{r,, include=FALSE}
WC2023_dataframe <- WC2023_dataframe %>%
  mutate(team.name = str_remove(team.name, " Women's"))

WC2023_dataframe <- WC2023_dataframe %>%
  mutate(possession_team.name = str_remove(possession_team.name, " Women's"))
```

# Descriptive data analysis 

 We begin by interpreting the elements of the data set.\
It is made up of multiple observations with different variables.\

## Analysis of successful shots according to country
   First, we look at the number of goals and shots in all matches for each team.\
Figure \ref{fig:fig1} shows a visualization of these results.\


```{r, include=FALSE}
#Number of goals and shots in all games for each team
shots_goals = WC2023_dataframe %>%
group_by(team.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 
```

```{r fig1 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig1}Diagram of the number of goals and shots in all matches for each team",fig.height=4.5}
#Let's make a graph
ggplot(shots_goals, aes(y = reorder(as.factor(team.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of goals and shots in all games for each team",
       y = "Team",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
```

It would be interesting to make this graph on the average number of goals and shots, as some teams have more games than others, distorting the results a little.

Figure \ref{fig:fig2} shows a visualization of the percentage of shots leading to a goal.\

```{r fig2 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig2}Diagram of the percentage of shots leading to a goal",fig.height=4,fig.align='center'}

ggplot(shots_goals, aes(y = reorder(as.factor(team.name), goals/shots*100))) +
  geom_bar(aes(x = goals/shots*100), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Percentage of shots leading to a goal",
       y = "Team",
       x = "Percentage")
```

\vspace{3pt}

We now would like to focus on France team.\
Figure \ref{fig:fig3} shows the results.\

```{r,include=F}
#Number of goals and shots in each match for both teams
shots_goals_all_matches = WC2023_dataframe %>%
group_by(match_id) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

```

```{r,include=F}
#We display ids of all France matches
Id_France = WC2023_dataframe%>% filter(team.name=="France")
match_ids_france=unique(Id_France$match_id)
print(match_ids_france)
```


```{r,include=F}

#For each French match, the total number of shots and goals for the 2 teams (France+adversary) is displayed.


france_goals <- shots_goals_all_matches %>%
  filter(match_id %in% match_ids_france)

print(france_goals)
```

```{r,include=F}

ggplot(france_goals, aes(y = as.factor(match_id))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of shots and goals for each French match",
       y = "Match_id",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
  
```



```{r,include=F}
ggplot(france_goals, aes(y = reorder(as.factor(match_id), goals/shots*100))) +
  geom_bar(aes(x = goals/shots*100), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Percentage of shots on goal in France matches",
       y = "Match",
       x = "Percentage")

#Il faut ordonner par date pour avoir l'évolution de la stratégie au cours de la compétition


```

```{r,include=F}

#We want to know which goals come from France
shots_goals_France_all_matches = WC2023_dataframe %>%
filter(team.name=="France") %>%
group_by(match_id) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

print (shots_goals_France_all_matches)

```



```{r fig3 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig3}Diagram of the number of shots and goals for each French and percentage",fig.height=2,fig.align='center'}

g1=ggplot(shots_goals_France_all_matches, aes(y = as.factor(match_id))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Numbers for French matches",
       y = "Match_id",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green")) 


g2=ggplot(shots_goals_France_all_matches, aes(y = as.factor(match_id))) +
  geom_bar(aes(x = goals/shots*100), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Percentage in France matches",
       y = "Match",
       x = "Percentage") 

#Order from bottom to top: 1st match to last
grid.arrange(g1,g2,ncol=2)
```
\clearpage
 
## Analysis of successful shots according to different variables  

We first look at the type of shots.\

```{r,include=F}
#Number of goals and shots in all matches by type of shot
shots_goals_formation = WC2023_dataframe %>%
group_by(shot.type.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

shots_goals_formation=shots_goals_formation[-c(5),]
```

```{r fig4 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig4}Diagram of the number of shots and goals for each type of shot",fig.height=2,fig.align='center'}
ggplot(shots_goals_formation, aes(y = reorder(as.factor(shot.type.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of shots and goals for each type of shot",
       y = "type",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
```


  We know would like to see if the technique of shot is significant.\

```{r,include=F}
#Number of shots and goals for each technique of shot
shots_goals_technique = WC2023_dataframe %>%
group_by(shot.technique.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 

#On enlève la ligne NA

shots_goals_technique=shots_goals_technique[-c(8),]
```

```{r fig5 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig5}Diagram of the number of shots and goals for each technique of shot",fig.height=2,fig.pos="h",fig.align='center'}

ggplot(shots_goals_technique, aes(y = reorder(as.factor(shot.technique.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of shots and goals for each technique of shot",
       y = "Technique of shot",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green"))
```

Finally, is the variable body_zone_used relevant ?

```{r,include=F}
#Number of goals and shots over all matches according to body zone used
shots_goals_part = WC2023_dataframe %>%
group_by(shot.body_part.name) %>% 
summarise(shots = sum(type.name=="Shot", na.rm = TRUE),
goals = sum(shot.outcome.name=="Goal", na.rm = TRUE)) 
shots_goals_part=shots_goals_part[-c(5),]

```

```{r fig6 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig6}Diagram of the number of goals and shots according to body zone used",fig.height=2,fig.pos="h"}

ggplot(shots_goals_part, aes(y = reorder(as.factor(shot.body_part.name), goals))) +
  geom_bar(aes(x = shots, fill = "shots"), stat = "identity", position = "dodge", width = 0.8) +
  geom_bar(aes(x = goals, fill = "goals"), stat = "identity", position = "dodge", width = 0.8) +
  labs(title = "Number of goals and shots according to body zone used",
       y = "Body zone",
       x = "Number") +
  scale_fill_manual(values = c("shots" = "brown", "goals" = "green")) 
```
\clearpage

# Models analysis

We wanted to create our own xG model. To do that we developed different models, finding the most relevant variables to predict goals.\

```{r,include=F}
#We create a df with just the shots

shots = WC2023_dataframe %>%
 filter(type.name=="Shot")
```

We run a logistic regression model: we want the output to be 0 or 1 depending on whether the shot turns into a goal.\

## First model : body part, technique, type of shot

The first model keeps the variables studied previously : body part, technique, type of shot. \


```{r,include=F}

df_model_1 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name)
df_model_1$shot.outcome.name <- ifelse(df_model_1$shot.outcome.name == "Goal", 1, 0)


```

```{r, include = F}
#Testing regression without interaction

glm.model_1<-glm(shot.outcome.name~ . ,data=df_model_1,family=binomial(link="logit"))
summary(glm.model_1)
```

```{r,include=F}
#We see a skewed shot.type.namePenalty because the corner modality is taken as reference and, given that there were no shots or goals during the competition, the model considers 100% success for the corner modality.


#We change the reference modality from “corner” to “penalty”.
df_model_1_modif=df_model_1
df_model_1_modif$shot.type.name <- relevel(as_factor(df_model_1_modif$shot.type.name), ref = "Penalty")

glm.model_1_modif <- glm(shot.outcome.name ~ . , data = df_model_1_modif, family = binomial(link = "logit"))
summary(glm.model_1_modif)

```

```{r,include=F}
#We can see that Open Play and Free Kick are less likely to result in a goal than a penalty kick. The corner remains distorted
```




```{r,include=F}
#R2 calculation:
R2_model1=1-(glm.model_1$deviance/glm.model_1$null.deviance)
print(R2_model1)

```
$R^2$ for the model without interaction is : \( `r R2_model1` \) \
```{r,include=F}
#Testing regression with interaction
glm.model_1_interac<-glm(shot.outcome.name~ .^2 ,data=df_model_1,family=binomial(link="logit"))
summary(glm.model_1_interac)
#A lot of NA because some interactions are impossible (e.g. volleyball restart on a penalty kick).

```


```{r,include=F}
#R2 calculation:
R2_model1_interac=1-(glm.model_1_interac$deviance/glm.model_1_interac$null.deviance)
print(R2_model1_interac)
```

$R^2$ for the model with interaction is : \( `r R2_model1_interac` \) .\

## Our target model : the expected goal variable

We now create a model composed of a single variable: the expected goal given in StatsBomb.\


```{r,include=F}
df_model_2 <- dplyr::select(shots,shot.statsbomb_xg,shot.outcome.name)
df_model_2$shot.outcome.name <- ifelse(df_model_2$shot.outcome.name == "Goal", 1, 0)

glm.model_2<-glm(shot.outcome.name~ . ,data=df_model_2,family=binomial(link="logit"))
summary(glm.model_2)

```

```{r,include=F}
R2_model2=1-(glm.model_2$deviance/glm.model_2$null.deviance)
print(R2_model2)

```

Our goal in creating the different models in this section is to find the most accurate model possible, which can have an $R^2$ close to this model (with only the expected goal as a variable), i.e. an $R^2$ close to : \( `r R2_model2` \).\

## Model 3 : Adding location.x and location.y

To do that we do the same logistical model as the first one but with the position added : location.x and location.y .\

```{r,include=F}

df_model_3 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y)
df_model_3$shot.outcome.name <- ifelse(df_model_3$shot.outcome.name == "Goal", 1, 0)

```

```{r,include=F}
#Model without interaction
glm.model_3<-glm(shot.outcome.name~ . ,data=df_model_3,family=binomial(link="logit"))
summary(glm.model_3)
```


```{r,include=F}
R2_model_3=1-(glm.model_3$deviance/glm.model_3$null.deviance)
print(R2_model_3)

```
We test a regression without interaction, and obtain an $R^2$ of : \( `r R2_model_3` \) .\


```{r,include=F}
#Model with interaction

glm.model_3_interac<-glm(shot.outcome.name~ .^2 ,data=df_model_3,family=binomial(link="logit"))
summary(glm.model_3_interac)
```

```{r,include=F}
R2_model_3_interac=1-(glm.model_3_interac$deviance/glm.model_3_interac$null.deviance)
print(R2_model_3_interac)
```

With interactions, we get an $R^2$ of : \( `r R2_model_3_interac` \).\

In this model, we targeted the main variables to obtain a good model and an $R^2$ as close to 1 as possible.

### Significance of variables ?


We run several tests to see which variables are significant in the model. \

```{r,echo=F}

model_simp<-glm(shot.outcome.name~ (location.x +location.y + shot.body_part.name+shot.type.name)^2 ,data=df_model_3,family=binomial(link="logit"))
anova(model_simp,glm.model_3_interac,test="Chisq")

```

We see that we can remove the technique because $p-value$>0.05 so we can accept the sub-model with a 95% level.\

```{r,include=F}
summary(model_simp)
```


```{r,include=F}
R2_model_simp=1-(model_simp$deviance/model_simp$null.deviance)
print(R2_model_simp)
```

For this sub-model without the technique variable we obtain an $R^2$ of : \( `r R2_model_simp` \)

The $R^2$ is no greater than for model 3 with interactions: this is normal because the $R^2$ favors models with many variables.\
We should look at other variables such as AIC, which is minimum for model 3 without interactions. \

```{r,include=F}
#We'll compare the fitted values (probability of being a goal), with the expected goals.
glm.model_3$fitted.values
#we have numbers between 0 and 1 so this is good.

```

### Comparison of norms

We now want to compare model 3 with and without interaction : the closer the 2-norm is to 0, the better the model. 


```{r,include=F}

#Display norm L2 for the model_3 without interaction

norm_L2_mod3 <- norm(glm.model_3$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod3)

#Display norm L2 for the model_3 with interactions 

norm_L2_mod3withinteractions <- norm(glm.model_3_interac$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod3withinteractions)


```

Norm L2 for the model_3 without interaction is equal to : \( `r norm_L2_mod3` \). \

The value for the model_3 with interactions is : \( `r norm_L2_mod3withinteractions` \).\

We find the same results as with the AIC criterion. This is consistent with the fact that $R^2$ favors models with many variables, so it's better to evaluate with AIC. We can conclude that the model 3 without interaction is best.

We do the same to compare model 1 with and without interaction.

```{r,include=F}

#Display norm L2 for the model_1 without interaction
norm_L2_mod1 <- norm(glm.model_1$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod1)

#Display norm L2 for the model_1 with interactions
norm_L2_mod1withinteractions <- norm(glm.model_1_interac$fitted.values - shots$shot.statsbomb_xg, type = "2")
print(norm_L2_mod1withinteractions)

```

The L2 norms are respectively :  \( `r norm_L2_mod1` \) and \( `r norm_L2_mod1withinteractions` \). \

Both models are less accurate than the 3rd one.


## Model 4 : adding the under_pressure variable

We now create a new model like the model_3, but adding a variable : under_pressure.\

### Test for significance of single variables

First, we test the significance of this new variable.

```{r,echo=F}
df_model_4 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y,under_pressure)
df_model_4$shot.outcome.name <- ifelse(df_model_4$shot.outcome.name == "Goal", 1, 0)

#Replace NA with “false” in the under_pressure column
df_model_4$under_pressure <- ifelse(is.na(df_model_4$under_pressure), FALSE, df_model_4$under_pressure)

#Testing the model with only the under_pressure variable
glm.model_4 <- glm(shot.outcome.name ~ under_pressure, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4)
```


We see that \( p_{\text{value}} < 0.05 \), so we reject $H_0$ : playing under pressure is significant. \

Estimated coefficients are negative, so playing under pressure reduces the probability of scoring. \




```{r,include=F}
glm.model_4_bodypart <- glm(shot.outcome.name ~ shot.body_part.name, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_bodypart)

#The probability of marking the head is lower than for other parts of the body.

```
```{r,include=F}
model_constant= glm(shot.outcome.name ~ 1, data = df_model_4, family = binomial(link = "logit"))
anova(model_constant,glm.model_4_bodypart,test="Chisq")
```



Testing the model with only the shot.body_part.name variable gives us a $p_{value}$ of :
\( `r round(anova(model_constant, glm.model_4_bodypart, test="Chisq")$Pr[2], digits = 3)` \). \
We reject $H_0$, the technique variable is significant.



We now want to test the model with only the shot.technique.name variable.


```{r,echo=F}

glm.model_4_technique <- glm(shot.outcome.name ~ shot.technique.name, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_technique)
```

The reference is backheel : all the other techniques are better, we have a lot of values close to 1, we could do a constant sub-model to see if this variable is significant. \

```{r,include=F}
model_constant= glm(shot.outcome.name ~ 1, data = df_model_4, family = binomial(link = "logit"))
anova(model_constant,glm.model_4_technique,test="Chisq")
```

We find a $p_{value}$ of \( `r round(anova(model_constant, glm.model_4_technique, test="Chisq")$Pr[2], digits = 3)` \).
We reject $H_0$, the technique variable is significant.



We are now testing the model with only the shot.type.name variable. We also run a sub-model test. \


```{r,include=F}

glm.model_4_type <- glm(shot.outcome.name ~ shot.type.name, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_type)

```

```{r,include=F}
anova(model_constant,glm.model_4_type,test="Chisq")
```
We find a $p_{value}$ of \( `r round(anova(model_constant, glm.model_4_type, test="Chisq")$Pr[2], digits = 3)` \).

The variable shot.type.name is significant, we reject $H_0$.



We do the same with the variable location.x : 

```{r,include=F}
glm.model_4_loc_x <- glm(shot.outcome.name ~ location.x, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_loc_x)
```

```{r,include=F}
anova(model_constant,glm.model_4_loc_x,test="Chisq")
```
We see a $p_{value}$ of : \( `r round(anova(model_constant, glm.model_4_loc_x, test="Chisq")$Pr[2], digits = 3)` \). <0.05 so location.x is highly significant.\


We check if the variable location.y is significant as well. \

```{r,include=F}
glm.model_4_loc_y <- glm(shot.outcome.name ~ location.y, data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_4_loc_y)
```

```{r,include=F}
anova(model_constant,glm.model_4_loc_y,test="Chisq")
```

The $p_{value}$ is : \( `r round(anova(model_constant, glm.model_4_loc_y, test="Chisq")$Pr[2], digits = 3)` \). > 0.05 so location.y is not significant.\


### Testing the complete model


The model is now tested with all the following variables: shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y,under_pressure.\


```{r,include=F}
#We test now with all the variable in the model 3+under_pressure 
glm.model_5 <- glm(shot.outcome.name ~ ., data = df_model_4, family = binomial(link = "logit"))
summary(glm.model_5)
AIC_model5=AIC(glm.model_5)
R2_model5=1-(glm.model_5$deviance/glm.model_5$null.deviance)
print(R2_model5)
```

We have an $R^2$ of \( `r R2_model5` \) which is good, but it's normal because it's a model with many variables. \

We also note a low AIC, which is equal to \texttt{`r AIC_model5`}.\

## Model 5 : adding the position of the goalkeeper


We create the same model as above, but adding the position of the goalkeeper. \

### Does the position of the guard in x and y improve our results ? 

First we test the model with only the location.x.GK variable.\

```{r,echo=F}
#Create a new df with all previous variables + goalkeeper position
df_model_6 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,location.y,under_pressure,location.x.GK,location.y.GK)
df_model_6$shot.outcome.name <- ifelse(df_model_6$shot.outcome.name == "Goal", 1, 0)

#Replace NA with “false” in the under_pressure column
df_model_6$under_pressure <- ifelse(is.na(df_model_6$under_pressure), FALSE, df_model_6$under_pressure)

#Testing the model with only the location.x.GK variable
glm.model_6_location.x.GK <- glm(shot.outcome.name ~ location.x.GK, data = df_model_6, family = binomial(link = "logit"))
summary(glm.model_6_location.x.GK)

AIC_model6_locx_GK=AIC(glm.model_6_location.x.GK)

print(1-(glm.model_6_location.x.GK$deviance/glm.model_6_location.x.GK$null.deviance))
```

Significant effect of goal position in x because both $p_{values}$ are lower than 0.5. \
The AIC value is low, equals to \texttt{`r AIC_model6_locx_GK`}.\

Then we do the same but with the location.y.GK variable.\

```{r,include=F}
#We test the model with only the variable location.y.GK
glm.model_6_location.y.GK <- glm(shot.outcome.name ~ location.y.GK, data = df_model_6, family = binomial(link = "logit"))
summary(glm.model_6_location.y.GK)

AIC_model6_locy_GK=AIC(glm.model_6_location.y.GK)


R2_model6loc_y_GK=1-(glm.model_6_location.y.GK$deviance/glm.model_6_location.y.GK$null.deviance)
print(R2_model6loc_y_GK)
```

We find that the variable for keeper position in y is significant as well.
AIC is slightly higher than for position in x, it's equal to \texttt{`r AIC_model6_locy_GK`}.\


### Complete model 



```{r,include=F}
#Testing the model with all df6 variables without interaction
glm.model_6 <- glm(shot.outcome.name ~ ., data = df_model_6, family = binomial(link = "logit"))
summary(glm.model_6)

AIC_model6=AIC(glm.model_6)

R2_model6=1-(glm.model_6$deviance/glm.model_6$null.deviance)
print(R2_model6)

```

For the model with all the preceding variables and without interaction, we find a very low AIC=\texttt{`r AIC_model6`}. \
We can conclude that this model is really good.\


## Last model : keeping all significant variables and removing location.y

```{r,include=F}
#We create a new df without location.y
df_model_7 <- dplyr::select(shots,shot.outcome.name,shot.body_part.name,shot.technique.name,shot.type.name,location.x,under_pressure,location.x.GK,location.y.GK)
df_model_7$shot.outcome.name <- ifelse(df_model_7$shot.outcome.name == "Goal", 1, 0)

#Replace NA with “false” in the under_pressure column
df_model_7$under_pressure <- ifelse(is.na(df_model_7$under_pressure), FALSE, df_model_7$under_pressure)

#Test the model with all variables
glm.model_7 <- glm(shot.outcome.name ~ ., data = df_model_7, family = binomial(link = "logit"))
summary(glm.model_7)

AIC_model7=AIC(glm.model_7)

R2_model7=1-(glm.model_7$deviance/glm.model_7$null.deviance)
print(R2_model7)

```

Since we found that location.y is noyt significant, we can remove it from the model.\ 

Without this variable, the AIC is even lower, at \texttt{`r AIC_model7`}.\

We can conclude that we have found our best model for now and it's composed of the variables : 

body_part, shot.technique, shot.type, location.x, under_pressure, location.x.GK, location.y.GK.\


```{r,echo=F}

#We want to compare our xg obtained with model 7 with those of statsbomb :

#We have a size problem between the two vectors: there are missing values
#We obtain the indices of missing observations for all variables in model 7

indices_obs_manquantes <- which(!complete.cases(df_model_7))
indices_non_manquants <- setdiff(1:nrow(df_model_7), indices_obs_manquantes)

#Only rows with no missing observations are kept
statsbomb_xg <- shots$shot.statsbomb_xg[indices_non_manquants]

df_plot <- data.frame(statsbomb_xg = statsbomb_xg, fitted = glm.model_7$fitted.values)

#We compare both
ggplot(df_plot, aes(x = seq_along(statsbomb_xg))) +
  geom_point(aes(y = statsbomb_xg), color = "blue") +
  geom_point(aes(y = fitted), color = "red") +
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "Observations",
       y = "Values") +
  theme_minimal()

ggplot(df_plot, aes(x = seq_along(statsbomb_xg))) +
  geom_point(aes(y = log(statsbomb_xg)), color = "blue") +
  geom_point(aes(y = log(fitted)), color = "red") +
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "Observations",
       y = "Values") +
  theme_minimal()


#We plot one against the other: xg against our fitted values
ggplot(df_plot, aes(x = statsbomb_xg, y = fitted)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "statsbomb_xg",
       y = "Fitted values") + 
  theme_minimal()  

#Same as transforming to log
ggplot(df_plot, aes(x = log(statsbomb_xg), y = log(fitted))) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  labs(title = "Comparison of statsbomb_xg and fitted values",
       x = "statsbomb_xg",
       y = "Fitted values") + 
  theme_minimal()  


```

We have a lot of values close to 0, so we do the log to make things clearer.

In log: there's a point (an observation) where we've overestimated the chance of scoring.
There are a few points at the bottom right where, on the contrary, we've underestimated the probability, but overall we've got a good prediction based on the Xg of bomb stats.


## Finding our best model with the AIC criterion

```{r,echo=F}
#Find the best model (that minimizes AIC)
stepAIC(glm.model_6, trace=FALSE)
```

We can see finally that y-positions are useless even for the goalkeeper, only x-positions are significant.

Our best model is composed of 4 variables : 
The technique of shot, the body part used and the positions in x for the player and the goalkeeper.

